ptfs262h@a0704:~/gpu$ nsys profile --stats=true --trace=cuda,nvtx ./MDBench-CP-gpusimple-NVCC-X86-SP 
WARNING: Device-side CUDA Event completion trace is currently enabled.
         This may increase runtime overhead and the likelihood of false
         dependencies across CUDA Streams. If you wish to avoid this, please
         disable the feature with --cuda-event-trace=false.
Collecting data...
Using temporary file: /tmp/3074961.alex/atoms_tmp.txt
Parameters:
        Force field: lj
        Kernel: GPU, MxN: 8x8, Vector width: 8
        SIMD Intrinsics: CUDA
        Super-clustering: no
        Data layout: AoS
        Floating-point precision: single
        Unit cells (nx, ny, nz): 32, 32, 32
        Domain box sizes (x, y, z): 5.374708e+01, 5.374708e+01, 5.374708e+01
        Periodic (x, y, z): 1, 1, 1
        Lattice size: 1.679596e+00
        Epsilon: 1.000000e+00
        Sigma: 1.000000e+00
        Temperature: 1.440000e+00
        RHO: 8.442000e-01
        Mass: 1.000000e+00
        Number of types: 4
        Number of timesteps: 200
        Report stats every (timesteps): 100
        Reneighbor every (timesteps): 20
        Sort atoms: no
        Single atom type: false
        Prune every (timesteps): 1000
        Output positions every (timesteps): 20
        Output velocities every (timesteps): 5
        Delta time (dt): 5.000000e-03
        Cutoff radius: 2.500000e+00
        Skin: 3.000000e-01
        Half neighbor lists: 0
        Processor frequency (GHz): 2.4000
----------------------------------------------------------------------------
step    temp            pressure
0       1.440001e+00    1.215639e+00
100     8.114799e-01    6.850460e-01
200     7.857317e-01    6.633096e-01
----------------------------------------------------------------------------
System: 131072 atoms 67574 ghost atoms, Steps: 200
TOTAL 1.11s

    | FORCE | NEIGH |BALANCE|FORWARD|REVERSE| UPDATE|  REST |  SETUP|
----|-------|-------|-------|-------|-------|-------|-------|-------|
 AVG|   0.05|   1.00|   0.00|   0.00|   0.00|   0.01|   0.06|   0.57|
 MIN|   0.05|   1.00|   0.00|   0.00|   0.00|   0.01|   0.06|   0.57|
 MAX|   0.05|   1.00|   0.00|   0.00|   0.00|   0.01|   0.06|   0.57|
----------------------------------------------------------------------------
Performance: 23.67 million atom updates per second
Generating '/tmp/3074961.alex/nsys-report-1134.qdstrm'
[1/7] [========================100%] report11.nsys-rep
[2/7] [========================100%] report11.sqlite
[3/7] Executing 'nvtx_sum' stats report
SKIPPED: /home/hpc/ptfs/ptfs262h/gpu/report11.sqlite does not contain NV Tools Extension (NVTX) data.
[4/7] Executing 'cuda_api_sum' stats report

 Time (%)  Total Time (ns)  Num Calls  Avg (ns)   Med (ns)  Min (ns)  Max (ns)   StdDev (ns)           Name         
 --------  ---------------  ---------  ---------  --------  --------  ---------  -----------  ----------------------
     57.6       61,950,131        791   78,318.7  37,895.0     8,788    435,285     90,958.8  cudaDeviceSynchronize 
     35.8       38,488,690        146  263,621.2  26,727.5     8,828  1,514,367    384,674.6  cudaMemcpy            
      3.1        3,288,259        791    4,157.1   3,347.0     3,016    218,945     10,060.0  cudaLaunchKernel      
      1.2        1,311,956         12  109,329.7  74,443.0     4,339    500,163    143,597.7  cudaFree              
      1.2        1,275,111         15   85,007.4  17,605.0     2,235    192,412     88,515.6  cudaMalloc            
      1.2        1,248,453        201    6,211.2   3,618.0     3,277    234,155     17,239.5  cudaMemset            
      0.0           14,449          1   14,449.0  14,449.0    14,449     14,449          0.0  cuCtxSynchronize      
      0.0            3,106          1    3,106.0   3,106.0     3,106      3,106          0.0  cudaDeviceReset       
      0.0              852          1      852.0     852.0       852        852          0.0  cuModuleGetLoadingMode

[5/7] Executing 'cuda_gpu_kern_sum' stats report

 Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                                                  Name                                                
 --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  ----------------------------------------------------------------------------------------------------
     78.4       43,060,019        201  214,229.0  214,335.0   208,478   220,319      1,955.5  computeForceLJCudaFullNeigh(int *, float *, float *, float *, int, float *, float *, int, int, int â€¦
     11.9        6,544,285        200   32,721.4   32,704.0    32,160    33,504        271.9  cudaInitialIntegrate_warp(float *, float *, float *, int *, int, float, float)                      
      6.0        3,269,419        200   16,347.1   16,255.0    15,775    17,631        405.3  cudaFinalIntegrate_warp(float *, float *, int *, int, float)                                        
      3.7        2,029,364        190   10,680.9   10,720.0    10,111    11,296        241.2  cudaUpdatePbc_warp(float *, int *, int *, int *, int *, int *, int, int, float, float, float)       

[6/7] Executing 'cuda_gpu_mem_time_sum' stats report

 Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Operation          
 --------  ---------------  -----  ---------  ---------  --------  ---------  -----------  ----------------------------
     50.0       16,800,929     22  763,678.6  731,916.0   705,692  1,144,698    109,880.7  [CUDA memcpy Device-to-Host]
     45.6       15,317,930    124  123,531.7    6,560.0     1,376  1,178,969    284,043.2  [CUDA memcpy Host-to-Device]
      4.5        1,496,889    201    7,447.2    7,424.0     7,168      9,312        183.3  [CUDA memset]               

[7/7] Executing 'cuda_gpu_mem_size_sum' stats report

 Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          
 ----------  -----  --------  --------  --------  --------  -----------  ----------------------------
  1,929.600    201     9.600     9.600     9.600     9.600        0.000  [CUDA memset]               
    212.686    124     1.715     0.067     0.000    13.354        3.747  [CUDA memcpy Host-to-Device]
    211.200     22     9.600     9.600     9.600     9.600        0.000  [CUDA memcpy Device-to-Host]